{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DnCNN-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/VictorMarchenkov/res_net/blob/master/DnCNN_keras.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fEDzjSCOsQ8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "По мативам DnCNN-keras-master с GitHub\n",
        "Пробую подготовит перенос на Colab ...\n"
      ]
    },
    {
      "metadata": {
        "id": "z2raupT_scxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YeRPj19wsdRF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QRgsFdcsdrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL \n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9JJzikJseLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCE-j6rBtEpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEUrOemUsQ8U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data.py\n",
        "\"\"\"\n",
        "Необходимо запускать для поготовки данных\n",
        "\"\"\"\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from multiprocessing import Pool\n",
        "\n",
        "patch_size, stride = 40, 10\n",
        "aug_times = 1\n",
        "\n",
        "def data_aug(img, mode=0):\n",
        "    \n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 2:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 3:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 4:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 5:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))\n",
        "    \n",
        "def gen_patches(file_name):\n",
        "\n",
        "    # read image\n",
        "    img = cv2.imread(file_name, 0)  # gray scale\n",
        "    h, w = img.shape\n",
        "    scales = [1, 0.9, 0.8, 0.7]\n",
        "    patches = []\n",
        "\n",
        "    for s in scales:\n",
        "        h_scaled, w_scaled = int(h*s),int(w*s)\n",
        "        img_scaled = cv2.resize(img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "        # extract patches\n",
        "        for i in range(0, h_scaled-patch_size+1, stride):\n",
        "            for j in range(0, w_scaled-patch_size+1, stride):\n",
        "                x = img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "                # data aug\n",
        "                for k in range(0, aug_times):\n",
        "                    x_aug = data_aug(x, mode=np.random.randint(0,8))\n",
        "                    patches.append(x_aug)\n",
        "    \n",
        "    return patches\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e__-jzIhxugF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip -q install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qf4dElXSsQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "src_dir = './drive/data/Train400/'\n",
        "save_dir = './drive/data/npy_data/'\n",
        "file_list = glob.glob(src_dir+'*.png')  # get name list of all .png files\n",
        "\n",
        "## num_threads = 16\n",
        "print('Start...')\n",
        "# initrialize\n",
        "res = []\n",
        "# generate patches\n",
        "## for i in range(0, len(file_list) ,num_threads):\n",
        "##     # use multi-process to speed up\n",
        "##     p = Pool(num_threads)\n",
        "##     patch = p.map(gen_patches,file_list[i:min(i+num_threads,len(file_list))])\n",
        "##     #patch = p.map(gen_patches,file_list[i:i+num_threads])\n",
        "##     for x in patch:\n",
        "##         res += x\n",
        "        \n",
        "##     print('Picture '+str(i)+' to '+str(i+num_threads)+' are finished...')\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    res += gen_patches(file_list[i])\n",
        "    \n",
        "# save to .npy\n",
        "res = np.array(res, dtype='uint8')\n",
        "print('Shape of result = ' + str(res.shape))\n",
        "print('Saving data...')\n",
        "if not os.path.exists(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "np.save(save_dir+'clean_patches.npy', res)\n",
        "print('Done.')       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3yYCkYVBuYoR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0OVCIt_hsQ8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#models.py\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import *\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Subtract\n",
        "\n",
        "\n",
        "def DnCNN():\n",
        "    \n",
        "    inpt = Input(shape=(None,None,1))\n",
        "    # 1st layer, Conv+relu\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(inpt)\n",
        "    x = Activation('relu')(x)\n",
        "    # 15 layers, Conv+BN+relu\n",
        "    for i in range(15):\n",
        "        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
        "        x = BatchNormalization(axis=-1, epsilon=1e-3)(x)\n",
        "        x = Activation('relu')(x)   \n",
        "    # last layer, Conv\n",
        "    x = Conv2D(filters=1, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n",
        "    x = Subtract()([inpt, x])   # input - noise\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRd2cA45sQ8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os, time, glob\n",
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from skimage.measure import compare_psnr, compare_ssim\n",
        "\n",
        "\n",
        "\n",
        "## Params\n",
        "## parser = argparse.ArgumentParser()\n",
        "## parser.add_argument('--model', default='DnCNN', type=str, help='choose a type of model')\n",
        "## parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
        "## parser.add_argument('--train_data', default='./drive/data/npy_data/clean_patches.npy', type=str, help='path of train data')\n",
        "## parser.add_argument('--test_dir', default='./drive/data/Test/Set68', type=str, help='directory of test dataset')\n",
        "## parser.add_argument('--sigma', default=25, type=int, help='noise level')\n",
        "## parser.add_argument('--epoch', default=50, type=int, help='number of train epoches')\n",
        "## parser.add_argument('--lr', default=1e-3, type=float, help='initial learning rate for Adam')\n",
        "## parser.add_argument('--save_every', default=5, type=int, help='save model at every x epoches')\n",
        "## parser.add_argument('--pretrain', default=None, type=str, help='path of pre-trained model')\n",
        "## parser.add_argument('--only_test', default=False, type=bool, help='train and test or only test')\n",
        "## args = parser.parse_args()\n",
        "\n",
        "model = 'DnCNN'\n",
        "batch_size = 128\n",
        "train_data = './drive/data/npy_data/clean_patches.npy'\n",
        "test_dir = './drive/data/Test/Set68'\n",
        "sigma = 25\n",
        "epoch = 50\n",
        "lr = 1e-3\n",
        "save_every = 5\n",
        "pretrain = None\n",
        "only_test = False\n",
        "\n",
        "\n",
        "if not only_test:\n",
        "    save_dir = './drive/snapshot/save_'+ model + '_' + 'sigma' + str(sigma) + '_' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) + '/'\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    # log\n",
        "    logging.basicConfig(level=logging.INFO,format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
        "                    datefmt='%Y %H:%M:%S',\n",
        "                    filename=save_dir+'info.log',\n",
        "                    filemode='w')\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(name)-6s: %(levelname)-6s %(message)s')\n",
        "    console.setFormatter(formatter)\n",
        "    logging.getLogger('').addHandler(console)\n",
        "    \n",
        "##     logging.info(args)\n",
        "    \n",
        "else:\n",
        "    save_dir = '/'.join(pretrain.split('/')[:-1]) + '/'\n",
        "\n",
        "      \n",
        "def load_train_data():\n",
        "    \n",
        "    logging.info('loading train data...')   \n",
        "    data = np.load(train_data)\n",
        "    logging.info('Size of train data: ({}, {}, {})'.format(data.shape[0],data.shape[1],data.shape[2]))\n",
        "    \n",
        "    return data\n",
        "\n",
        "def step_decay(epoch):\n",
        "    lr = 1e-3\n",
        "    initial_lr = lr\n",
        "    if epoch<50:\n",
        "        lr = initial_lr\n",
        "    else:\n",
        "        lr = initial_lr/10\n",
        "    \n",
        "    return lr\n",
        "\n",
        "def train_datagen(y_, batch_size=8):\n",
        "    \n",
        "    # y_ is the tensor of clean patches\n",
        "    indices = list(range(y_.shape[0]))\n",
        "    while(True):\n",
        "        np.random.shuffle(indices)    # shuffle\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            ge_batch_y = y_[indices[i:i+batch_size]]\n",
        "            noise =  np.random.normal(0, sigma/255.0, ge_batch_y.shape)    # noise\n",
        "            #noise =  K.random_normal(ge_batch_y.shape, mean=0, stddev=args.sigma/255.0)\n",
        "            ge_batch_x = ge_batch_y + noise  # input image = clean image + noise\n",
        "            yield ge_batch_x, ge_batch_y\n",
        "        \n",
        "def train():\n",
        "    \n",
        "    data = load_train_data()\n",
        "    data = data.reshape((data.shape[0],data.shape[1],data.shape[2],1))\n",
        "    data = data.astype('float32')/255.0\n",
        "    # model selection\n",
        "    if pretrain:\n",
        "        model_ = load_model(pretrain, compile=False)\n",
        "    else:   \n",
        "        if model == 'DnCNN':\n",
        "            model_ = DnCNN()\n",
        "    # compile the model\n",
        "    model_.compile(optimizer=Adam(), loss=['mse'])\n",
        "    \n",
        "    # use call back functions\n",
        "    ckpt = ModelCheckpoint(save_dir+'/model_{epoch:02d}.h5', monitor='val_loss', \n",
        "                    verbose=0, period=save_every)\n",
        "    csv_logger = CSVLogger(save_dir+'/log.csv', append=True, separator=',')\n",
        "    lr = LearningRateScheduler(step_decay)\n",
        "    # train \n",
        "    history = model_.fit_generator(train_datagen(data, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(data)//batch_size, epochs=epoch, verbose=1, \n",
        "                    callbacks=[ckpt, csv_logger, lr])\n",
        "    \n",
        "    return model_\n",
        "\n",
        "def test(model_):\n",
        "    \n",
        "    print('Start to test on {}'.format(test_dir))\n",
        "    out_dir = save_dir + test_dir.split('/')[-1] + '/'\n",
        "    if not os.path.exists(out_dir):\n",
        "            os.mkdir(out_dir)\n",
        "            \n",
        "    name = []\n",
        "    psnr = []\n",
        "    ssim = []\n",
        "    file_list = glob.glob('{}/*.png'.format(test_dir))\n",
        "    for file in file_list:\n",
        "        # read image\n",
        "        img_clean = np.array(Image.open(file), dtype='float32') / 255.0\n",
        "        img_test = img_clean + np.random.normal(0, sigma/255.0, img_clean.shape)\n",
        "        img_test = img_test.astype('float32')\n",
        "        # predict\n",
        "        x_test = img_test.reshape(1, img_test.shape[0], img_test.shape[1], 1) \n",
        "        y_predict = model_.predict(x_test)\n",
        "        # calculate numeric metrics\n",
        "        img_out = y_predict.reshape(img_clean.shape)\n",
        "        img_out = np.clip(img_out, 0, 1)\n",
        "        psnr_noise, psnr_denoised = compare_psnr(img_clean, img_test), compare_psnr(img_clean, img_out)\n",
        "        ssim_noise, ssim_denoised = compare_ssim(img_clean, img_test), compare_ssim(img_clean, img_out)\n",
        "        psnr.append(psnr_denoised)\n",
        "        ssim.append(ssim_denoised)\n",
        "        # save images\n",
        "        filename = file.split('/')[-1].split('.')[0]    # get the name of image file\n",
        "        name.append(filename)\n",
        "        img_test = Image.fromarray((img_test*255).astype('uint8'))\n",
        "        img_test.save(out_dir+filename+'_sigma'+'{}_psnr{:.2f}.png'.format(sigma, psnr_noise))\n",
        "        img_out = Image.fromarray((img_out*255).astype('uint8')) \n",
        "        img_out.save(out_dir+filename+'_psnr{:.2f}.png'.format(psnr_denoised))\n",
        "    \n",
        "    psnr_avg = sum(psnr)/len(psnr)\n",
        "    ssim_avg = sum(ssim)/len(ssim)\n",
        "    name.append('Average')\n",
        "    psnr.append(psnr_avg)\n",
        "    ssim.append(ssim_avg)\n",
        "    print('Average PSNR = {0:.2f}, SSIM = {1:.2f}'.format(psnr_avg, ssim_avg))\n",
        "    \n",
        "    pd.DataFrame({'name':np.array(name), 'psnr':np.array(psnr), 'ssim':np.array(ssim)}).to_csv(out_dir+'/metrics.csv', index=True)\n",
        "    \n",
        " \n",
        "    \n",
        "\n",
        "model_ = train()\n",
        "test(model_)       \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwcJ85J7sQ8n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model(pretrain, compile=False)\n",
        "test(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}